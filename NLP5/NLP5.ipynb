{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ali\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 2.3826 - accuracy: 0.2910\n",
            "Epoch 2/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 2.0622 - accuracy: 0.3828\n",
            "Epoch 3/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.9038 - accuracy: 0.4340\n",
            "Epoch 4/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.7901 - accuracy: 0.4679\n",
            "Epoch 5/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.7035 - accuracy: 0.4956\n",
            "Epoch 6/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.6385 - accuracy: 0.5141\n",
            "Epoch 7/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.5875 - accuracy: 0.5297\n",
            "Epoch 8/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.5487 - accuracy: 0.5394\n",
            "Epoch 9/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.5157 - accuracy: 0.5493\n",
            "Epoch 10/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.4865 - accuracy: 0.5585\n",
            "Epoch 11/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.4606 - accuracy: 0.5634\n",
            "Epoch 12/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.4411 - accuracy: 0.5704\n",
            "Epoch 13/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.4218 - accuracy: 0.5732\n",
            "Epoch 14/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.4050 - accuracy: 0.5786\n",
            "Epoch 15/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.3919 - accuracy: 0.5819\n",
            "Epoch 16/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.3760 - accuracy: 0.5861\n",
            "Epoch 17/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.3623 - accuracy: 0.5888\n",
            "Epoch 18/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.3504 - accuracy: 0.5909\n",
            "Epoch 19/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.3427 - accuracy: 0.5944\n",
            "Epoch 20/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.3318 - accuracy: 0.5968\n",
            "Epoch 21/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.3244 - accuracy: 0.5982\n",
            "Epoch 22/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.3153 - accuracy: 0.6003\n",
            "Epoch 23/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.3092 - accuracy: 0.6026\n",
            "Epoch 24/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.3006 - accuracy: 0.6057\n",
            "Epoch 25/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2967 - accuracy: 0.6055\n",
            "Epoch 26/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2888 - accuracy: 0.6086\n",
            "Epoch 27/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.2843 - accuracy: 0.6080\n",
            "Epoch 28/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2790 - accuracy: 0.6100\n",
            "Epoch 29/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2765 - accuracy: 0.6093\n",
            "Epoch 30/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2669 - accuracy: 0.6134\n",
            "Epoch 31/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2665 - accuracy: 0.6132\n",
            "Epoch 32/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.2593 - accuracy: 0.6136\n",
            "Epoch 33/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2560 - accuracy: 0.6171\n",
            "Epoch 34/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.2516 - accuracy: 0.6154\n",
            "Epoch 35/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2517 - accuracy: 0.6162\n",
            "Epoch 36/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.2454 - accuracy: 0.6180\n",
            "Epoch 37/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2451 - accuracy: 0.6185\n",
            "Epoch 38/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2424 - accuracy: 0.6189\n",
            "Epoch 39/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2385 - accuracy: 0.6189\n",
            "Epoch 40/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2369 - accuracy: 0.6192\n",
            "Epoch 41/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2347 - accuracy: 0.6221\n",
            "Epoch 42/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2325 - accuracy: 0.6206\n",
            "Epoch 43/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2295 - accuracy: 0.6209\n",
            "Epoch 44/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2298 - accuracy: 0.6222\n",
            "Epoch 45/50\n",
            "1384/1384 [==============================] - 8s 6ms/step - loss: 1.2296 - accuracy: 0.6209\n",
            "Epoch 46/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2232 - accuracy: 0.6231\n",
            "Epoch 47/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2247 - accuracy: 0.6237\n",
            "Epoch 48/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2267 - accuracy: 0.6220\n",
            "Epoch 49/50\n",
            "1384/1384 [==============================] - 7s 5ms/step - loss: 1.2199 - accuracy: 0.6248\n",
            "Epoch 50/50\n",
            "1384/1384 [==============================] - 8s 5ms/step - loss: 1.2182 - accuracy: 0.6226\n",
            "Epoch 1/50\n",
            "1384/1384 [==============================] - 27s 19ms/step - loss: 2.3973 - accuracy: 0.2835\n",
            "Epoch 2/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 2.0250 - accuracy: 0.3938\n",
            "Epoch 3/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.8398 - accuracy: 0.4538\n",
            "Epoch 4/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.7130 - accuracy: 0.4910\n",
            "Epoch 5/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.6172 - accuracy: 0.5168\n",
            "Epoch 6/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.5408 - accuracy: 0.5398\n",
            "Epoch 7/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.4799 - accuracy: 0.5565\n",
            "Epoch 8/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.4288 - accuracy: 0.5696\n",
            "Epoch 9/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.3823 - accuracy: 0.5823\n",
            "Epoch 10/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.3447 - accuracy: 0.5929\n",
            "Epoch 11/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.3094 - accuracy: 0.6023\n",
            "Epoch 12/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.2756 - accuracy: 0.6139\n",
            "Epoch 13/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.2477 - accuracy: 0.6210\n",
            "Epoch 14/50\n",
            "1384/1384 [==============================] - 25s 18ms/step - loss: 1.2205 - accuracy: 0.6259\n",
            "Epoch 15/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.1970 - accuracy: 0.6326\n",
            "Epoch 16/50\n",
            "1384/1384 [==============================] - 26s 18ms/step - loss: 1.1717 - accuracy: 0.6403\n",
            "Epoch 17/50\n",
            "1384/1384 [==============================] - 26s 18ms/step - loss: 1.1487 - accuracy: 0.6476\n",
            "Epoch 18/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.1292 - accuracy: 0.6504\n",
            "Epoch 19/50\n",
            "1384/1384 [==============================] - 27s 20ms/step - loss: 1.1105 - accuracy: 0.6572\n",
            "Epoch 20/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.0889 - accuracy: 0.6643\n",
            "Epoch 21/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.0720 - accuracy: 0.6682\n",
            "Epoch 22/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.0534 - accuracy: 0.6730\n",
            "Epoch 23/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.0384 - accuracy: 0.6781\n",
            "Epoch 24/50\n",
            "1384/1384 [==============================] - 26s 19ms/step - loss: 1.0224 - accuracy: 0.6817\n",
            "Epoch 25/50\n",
            "1384/1384 [==============================] - 27s 20ms/step - loss: 1.0070 - accuracy: 0.6872\n",
            "Epoch 26/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.9926 - accuracy: 0.6904\n",
            "Epoch 27/50\n",
            "1384/1384 [==============================] - 27s 20ms/step - loss: 0.9774 - accuracy: 0.6934\n",
            "Epoch 28/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.9637 - accuracy: 0.6998\n",
            "Epoch 29/50\n",
            "1384/1384 [==============================] - 27s 20ms/step - loss: 0.9511 - accuracy: 0.7027\n",
            "Epoch 30/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.9365 - accuracy: 0.7068\n",
            "Epoch 31/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.9241 - accuracy: 0.7127\n",
            "Epoch 32/50\n",
            "1384/1384 [==============================] - 29s 21ms/step - loss: 0.9123 - accuracy: 0.7151\n",
            "Epoch 33/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.9015 - accuracy: 0.7184\n",
            "Epoch 34/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8917 - accuracy: 0.7207\n",
            "Epoch 35/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8800 - accuracy: 0.7233\n",
            "Epoch 36/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8722 - accuracy: 0.7295\n",
            "Epoch 37/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8602 - accuracy: 0.7315\n",
            "Epoch 38/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8498 - accuracy: 0.7335\n",
            "Epoch 39/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8413 - accuracy: 0.7356\n",
            "Epoch 40/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8320 - accuracy: 0.7408\n",
            "Epoch 41/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8219 - accuracy: 0.7437\n",
            "Epoch 42/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8170 - accuracy: 0.7434\n",
            "Epoch 43/50\n",
            "1384/1384 [==============================] - 29s 21ms/step - loss: 0.8076 - accuracy: 0.7475\n",
            "Epoch 44/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.8000 - accuracy: 0.7504\n",
            "Epoch 45/50\n",
            "1384/1384 [==============================] - 28s 21ms/step - loss: 0.7916 - accuracy: 0.7525\n",
            "Epoch 46/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.7834 - accuracy: 0.7544\n",
            "Epoch 47/50\n",
            "1384/1384 [==============================] - 28s 21ms/step - loss: 0.7758 - accuracy: 0.7574\n",
            "Epoch 48/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.7701 - accuracy: 0.7581\n",
            "Epoch 49/50\n",
            "1384/1384 [==============================] - 28s 21ms/step - loss: 0.7637 - accuracy: 0.7599\n",
            "Epoch 50/50\n",
            "1384/1384 [==============================] - 28s 20ms/step - loss: 0.7603 - accuracy: 0.7600\n",
            "Epoch 1/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 7.4773 - accuracy: 0.0480\n",
            "Epoch 2/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 7.0191 - accuracy: 0.0545\n",
            "Epoch 3/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 6.7449 - accuracy: 0.0559\n",
            "Epoch 4/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 6.3599 - accuracy: 0.0628\n",
            "Epoch 5/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 5.9045 - accuracy: 0.0944\n",
            "Epoch 6/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 5.4005 - accuracy: 0.1648\n",
            "Epoch 7/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 4.8684 - accuracy: 0.2510\n",
            "Epoch 8/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 4.3203 - accuracy: 0.3520\n",
            "Epoch 9/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 3.7862 - accuracy: 0.4540\n",
            "Epoch 10/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 3.2756 - accuracy: 0.5563\n",
            "Epoch 11/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 2.7962 - accuracy: 0.6418\n",
            "Epoch 12/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 2.3591 - accuracy: 0.7167\n",
            "Epoch 13/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 1.9638 - accuracy: 0.7741\n",
            "Epoch 14/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 1.6248 - accuracy: 0.8266\n",
            "Epoch 15/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 1.3462 - accuracy: 0.8620\n",
            "Epoch 16/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 1.1096 - accuracy: 0.8940\n",
            "Epoch 17/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.9168 - accuracy: 0.9210\n",
            "Epoch 18/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.7550 - accuracy: 0.9437\n",
            "Epoch 19/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.6216 - accuracy: 0.9596\n",
            "Epoch 20/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.5151 - accuracy: 0.9709\n",
            "Epoch 21/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.4261 - accuracy: 0.9790\n",
            "Epoch 22/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.3504 - accuracy: 0.9869\n",
            "Epoch 23/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.2936 - accuracy: 0.9904\n",
            "Epoch 24/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.2435 - accuracy: 0.9938\n",
            "Epoch 25/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.2057 - accuracy: 0.9954\n",
            "Epoch 26/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.1706 - accuracy: 0.9981\n",
            "Epoch 27/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.1438 - accuracy: 0.9985\n",
            "Epoch 28/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.1221 - accuracy: 0.9991\n",
            "Epoch 29/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.1037 - accuracy: 0.9995\n",
            "Epoch 30/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0896 - accuracy: 0.9998\n",
            "Epoch 31/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0779 - accuracy: 0.9998\n",
            "Epoch 32/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0674 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0589 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0517 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "182/182 [==============================] - 2s 10ms/step - loss: 0.0459 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0407 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0364 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0297 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0322 - accuracy: 0.9997\n",
            "Epoch 41/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0293 - accuracy: 0.9997\n",
            "Epoch 42/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0220 - accuracy: 0.9998\n",
            "Epoch 44/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0192 - accuracy: 0.9998\n",
            "Epoch 45/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 0.0132 - accuracy: 0.9998\n",
            "Epoch 50/50\n",
            "182/182 [==============================] - 2s 9ms/step - loss: 1.1222 - accuracy: 0.7640\n",
            "Epoch 1/50\n",
            "182/182 [==============================] - 6s 25ms/step - loss: 7.4505 - accuracy: 0.0537\n",
            "Epoch 2/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 7.0421 - accuracy: 0.0547\n",
            "Epoch 3/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 6.8883 - accuracy: 0.0550\n",
            "Epoch 4/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 6.7347 - accuracy: 0.0552\n",
            "Epoch 5/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 6.5393 - accuracy: 0.0547\n",
            "Epoch 6/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 6.3428 - accuracy: 0.0557\n",
            "Epoch 7/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 6.1304 - accuracy: 0.0583\n",
            "Epoch 8/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 5.9095 - accuracy: 0.0616\n",
            "Epoch 9/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 5.6936 - accuracy: 0.0667\n",
            "Epoch 10/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 5.4694 - accuracy: 0.0778\n",
            "Epoch 11/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 5.2515 - accuracy: 0.0848\n",
            "Epoch 12/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 5.0349 - accuracy: 0.1032\n",
            "Epoch 13/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 4.8461 - accuracy: 0.1223\n",
            "Epoch 14/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 4.6781 - accuracy: 0.1369\n",
            "Epoch 15/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 4.4623 - accuracy: 0.1682\n",
            "Epoch 16/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 4.2574 - accuracy: 0.1977\n",
            "Epoch 17/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 4.0679 - accuracy: 0.2346\n",
            "Epoch 18/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 3.8888 - accuracy: 0.2634\n",
            "Epoch 19/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 3.7130 - accuracy: 0.3031\n",
            "Epoch 20/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 3.5304 - accuracy: 0.3444\n",
            "Epoch 21/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 3.3547 - accuracy: 0.3771\n",
            "Epoch 22/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 3.1885 - accuracy: 0.4101\n",
            "Epoch 23/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 3.0251 - accuracy: 0.4485\n",
            "Epoch 24/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 2.8691 - accuracy: 0.4837\n",
            "Epoch 25/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 2.7134 - accuracy: 0.5183\n",
            "Epoch 26/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 2.5623 - accuracy: 0.5512\n",
            "Epoch 27/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 2.4129 - accuracy: 0.5811\n",
            "Epoch 28/50\n",
            "182/182 [==============================] - 5s 25ms/step - loss: 2.2692 - accuracy: 0.6102\n",
            "Epoch 29/50\n",
            "182/182 [==============================] - 5s 25ms/step - loss: 2.1391 - accuracy: 0.6398\n",
            "Epoch 30/50\n",
            "182/182 [==============================] - 5s 25ms/step - loss: 2.0114 - accuracy: 0.6657\n",
            "Epoch 31/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.8913 - accuracy: 0.6838\n",
            "Epoch 32/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.7717 - accuracy: 0.7084\n",
            "Epoch 33/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.6637 - accuracy: 0.7285\n",
            "Epoch 34/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.5637 - accuracy: 0.7528\n",
            "Epoch 35/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.4623 - accuracy: 0.7671\n",
            "Epoch 36/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.3717 - accuracy: 0.7832\n",
            "Epoch 37/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.2813 - accuracy: 0.8027\n",
            "Epoch 38/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.2063 - accuracy: 0.8170\n",
            "Epoch 39/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.1229 - accuracy: 0.8355\n",
            "Epoch 40/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 1.0487 - accuracy: 0.8464\n",
            "Epoch 41/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.9802 - accuracy: 0.8605\n",
            "Epoch 42/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.9174 - accuracy: 0.8718\n",
            "Epoch 43/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 0.8574 - accuracy: 0.8866\n",
            "Epoch 44/50\n",
            "182/182 [==============================] - 4s 25ms/step - loss: 0.7974 - accuracy: 0.8963\n",
            "Epoch 45/50\n",
            "182/182 [==============================] - 5s 25ms/step - loss: 0.7396 - accuracy: 0.9052\n",
            "Epoch 46/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.6892 - accuracy: 0.9150\n",
            "Epoch 47/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.6377 - accuracy: 0.9240\n",
            "Epoch 48/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.5935 - accuracy: 0.9333\n",
            "Epoch 49/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.5526 - accuracy: 0.9379\n",
            "Epoch 50/50\n",
            "182/182 [==============================] - 4s 24ms/step - loss: 0.5212 - accuracy: 0.9432\n",
            "Character-based prediction using Simple RNN Accuracy: 0.6368052363395691\n",
            "Character-based prediction using LSTM Accuracy: 0.7831450700759888\n",
            "Word-based prediction using Simple RNN Accuracy: 0.8847410678863525\n",
            "Word-based prediction using LSTM Accuracy: 0.9566488862037659\n",
            "Predicted next word: statement\n",
            "Predicted sequence: gex_centain_rouncig_version_number_incremented_format_long_extension_interpreter_programming_languag\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import wikipedia\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences as keras_pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, SimpleRNN, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Fetch Wikipedia articles\n",
        "def fetch_articles(titles):\n",
        "    wikipedia.set_lang(\"en\")\n",
        "    articles = {}\n",
        "    for title in titles:\n",
        "        articles[title] = wikipedia.page(title).content\n",
        "    return articles\n",
        "\n",
        "titles = ['Python (programming language)', 'Python (snake)']\n",
        "articles = fetch_articles(titles)\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W|\\d', ' ', text)\n",
        "    text = text.lower()\n",
        "    text = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    return text\n",
        "\n",
        "preprocessed_articles = {title: preprocess_text(article) for title, article in articles.items()}\n",
        "\n",
        "# Character-level preprocessing\n",
        "def char_level_preprocessing(text):\n",
        "    text = ' '.join(preprocess_text(text))\n",
        "    text = text.replace(' ', '_')\n",
        "    return list(text)\n",
        "\n",
        "char_tokenized_articles = {title: char_level_preprocessing(article) for title, article in articles.items()}\n",
        "\n",
        "# Character tokenizer\n",
        "char_tokenizer = Tokenizer(char_level=True)\n",
        "char_tokenizer.fit_on_texts(char_tokenized_articles.values())\n",
        "\n",
        "# Convert text to sequences of characters\n",
        "char_sequences = char_tokenizer.texts_to_sequences(char_tokenized_articles.values())\n",
        "\n",
        "# Generate character sequences\n",
        "def generate_char_sequences(text, seq_length):\n",
        "    return [(text[i:i+seq_length], text[i+seq_length]) for i in range(len(text) - seq_length)]\n",
        "\n",
        "char_sequences = [generate_char_sequences(seq, max_sequence_length) for seq in char_sequences]\n",
        "char_sequences = [item for sublist in char_sequences for item in sublist]\n",
        "\n",
        "# Separate input and target characters\n",
        "char_input_sequences = [seq[0] for seq in char_sequences]\n",
        "char_target_characters = [seq[1] for seq in char_sequences]\n",
        "\n",
        "# Pad character sequences\n",
        "char_input_sequences = keras_pad_sequences(char_input_sequences, maxlen=max_sequence_length-1, padding='pre')\n",
        "char_target_characters = np.array(char_target_characters)\n",
        "\n",
        "# Word tokenizer\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(preprocessed_articles.values())\n",
        "word_sequences = word_tokenizer.texts_to_sequences(preprocessed_articles.values())\n",
        "\n",
        "# Generate word sequences\n",
        "def generate_word_sequences(text, seq_length):\n",
        "    return [(text[i:i+seq_length], text[i+seq_length]) for i in range(len(text) - seq_length)]\n",
        "\n",
        "word_sequences = [generate_word_sequences(seq, max_sequence_length) for seq in word_sequences]\n",
        "word_sequences = [item for sublist in word_sequences for item in sublist]\n",
        "\n",
        "# Separate input and target words\n",
        "word_input_sequences = [seq[0] for seq in word_sequences]\n",
        "word_target_words = [seq[1] for seq in word_sequences]\n",
        "\n",
        "# Pad word sequences\n",
        "word_input_sequences = keras_pad_sequences(word_input_sequences, maxlen=max_sequence_length-1, padding='pre')\n",
        "word_target_words = np.array(word_target_words)\n",
        "\n",
        "# Define models\n",
        "def create_simple_rnn_model(vocab_size, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length-1),\n",
        "        SimpleRNN(units=100),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_lstm_model(vocab_size, max_sequence_length):\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length-1),\n",
        "        LSTM(units=100),\n",
        "        Dense(vocab_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Character-based models\n",
        "char_vocab_size = len(char_tokenizer.word_index) + 1\n",
        "char_simple_rnn_model = create_simple_rnn_model(char_vocab_size, max_sequence_length)\n",
        "char_lstm_model = create_lstm_model(char_vocab_size, max_sequence_length)\n",
        "\n",
        "# Train character-based models\n",
        "char_simple_rnn_model.fit(char_input_sequences, char_target_characters, epochs=50, verbose=1)\n",
        "char_lstm_model.fit(char_input_sequences, char_target_characters, epochs=50, verbose=1)\n",
        "\n",
        "# Word-based models\n",
        "word_vocab_size = len(word_tokenizer.word_index) + 1\n",
        "word_simple_rnn_model = create_simple_rnn_model(word_vocab_size, max_sequence_length)\n",
        "word_lstm_model = create_lstm_model(word_vocab_size, max_sequence_length)\n",
        "\n",
        "# Train word-based models\n",
        "word_simple_rnn_model.fit(word_input_sequences, word_target_words, epochs=50, verbose=1)\n",
        "word_lstm_model.fit(word_input_sequences, word_target_words, epochs=50, verbose=1)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, input_sequences, target_words):\n",
        "    loss, accuracy = model.evaluate(input_sequences, target_words, verbose=0)\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate models\n",
        "char_simple_rnn_accuracy = evaluate_model(char_simple_rnn_model, char_input_sequences, char_target_characters)\n",
        "char_lstm_accuracy = evaluate_model(char_lstm_model, char_input_sequences, char_target_characters)\n",
        "word_simple_rnn_accuracy = evaluate_model(word_simple_rnn_model, word_input_sequences, word_target_words)\n",
        "word_lstm_accuracy = evaluate_model(word_lstm_model, word_input_sequences, word_target_words)\n",
        "\n",
        "print(\"Character-based prediction using Simple RNN Accuracy:\", char_simple_rnn_accuracy)\n",
        "print(\"Character-based prediction using LSTM Accuracy:\", char_lstm_accuracy)\n",
        "print(\"Word-based prediction using Simple RNN Accuracy:\", word_simple_rnn_accuracy)\n",
        "print(\"Word-based prediction using LSTM Accuracy:\", word_lstm_accuracy)\n",
        "\n",
        "# Prediction functions\n",
        "def preprocess_input_text(input_text):\n",
        "    preprocessed_text = preprocess_text(input_text)\n",
        "    return preprocessed_text\n",
        "\n",
        "def generate_input_sequences(preprocessed_text, max_sequence_length, tokenizer):\n",
        "    input_sequences = tokenizer.texts_to_sequences([preprocessed_text])[0]\n",
        "    input_sequences = keras_pad_sequences([input_sequences], maxlen=max_sequence_length-1, padding='pre')\n",
        "    return input_sequences\n",
        "\n",
        "def predict_next_word_lstm_word_based(input_text, model, max_sequence_length, tokenizer):\n",
        "    preprocessed_text = preprocess_input_text(input_text)\n",
        "    input_sequences = generate_input_sequences(preprocessed_text, max_sequence_length, tokenizer)\n",
        "    predicted_probabilities = model.predict(input_sequences, verbose=0)[0]\n",
        "    predicted_index = np.argmax(predicted_probabilities)\n",
        "    predicted_word = tokenizer.index_word[predicted_index]\n",
        "    return predicted_word\n",
        "\n",
        "def predict_next_character_lstm_char_based(input_text, model, max_sequence_length, tokenizer, max_chars=100):\n",
        "    input_text = list(input_text)\n",
        "    generated_text = input_text\n",
        "\n",
        "    for _ in range(max_chars):\n",
        "        input_sequences = tokenizer.texts_to_sequences([''.join(generated_text)])[0]\n",
        "        input_sequences = keras_pad_sequences([input_sequences], maxlen=max_sequence_length-1, padding='pre')\n",
        "        predicted_probabilities = model.predict(input_sequences, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probabilities)\n",
        "        predicted_character = tokenizer.index_word.get(predicted_index, '')\n",
        "\n",
        "        generated_text.append(predicted_character)\n",
        "\n",
        "        if predicted_character == ' ' or len(generated_text) >= max_chars:\n",
        "            break\n",
        "\n",
        "    return ''.join(generated_text).strip()\n",
        "\n",
        "# Example usage for LSTM word-based prediction\n",
        "input_text = \"Python is a popular\"\n",
        "predicted_word = predict_next_word_lstm_word_based(input_text, word_lstm_model, max_sequence_length, word_tokenizer)\n",
        "print(\"Predicted next word:\", predicted_word)\n",
        "\n",
        "# Example usage for LSTM character-based prediction\n",
        "input_text = \"ge\"\n",
        "predicted_sequence = predict_next_character_lstm_char_based(input_text, char_lstm_model, max_sequence_length, char_tokenizer)\n",
        "print(\"Predicted sequence:\", predicted_sequence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted sequence: lant_switter_two_like_reticulated_python_southeast_support_python_support_optime_name_type_constraint_c\n"
          ]
        }
      ],
      "source": [
        "def predict_next_character_lstm_char_based(input_text, model, max_sequence_length, tokenizer, max_chars=100):\n",
        "    input_text = list(input_text)\n",
        "    generated_text = input_text\n",
        "\n",
        "    for _ in range(max_chars):\n",
        "        # Convert the generated text to sequences\n",
        "        input_sequences = tokenizer.texts_to_sequences([''.join(generated_text)])[0]\n",
        "        input_sequences = keras_pad_sequences([input_sequences], maxlen=max_sequence_length-1, padding='pre')\n",
        "        \n",
        "        # Predict the next character\n",
        "        predicted_probabilities = model.predict(input_sequences, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probabilities)\n",
        "        \n",
        "        # Get the predicted character\n",
        "        predicted_character = tokenizer.index_word.get(predicted_index, '')\n",
        "        \n",
        "        # Append the predicted character to the generated text\n",
        "        generated_text.append(predicted_character)\n",
        "\n",
        "        # Check if the generated text forms a complete word\n",
        "        generated_word = ''.join(generated_text).strip()\n",
        "        if generated_word in tokenizer.word_index:\n",
        "            break\n",
        "\n",
        "        # Break the loop if a space is detected\n",
        "        if predicted_character == ' ':\n",
        "            break\n",
        "\n",
        "    return generated_word\n",
        "\n",
        "# Example usage for LSTM character-based prediction\n",
        "input_text = \"lan\"\n",
        "predicted_sequence = predict_next_character_lstm_char_based(input_text, char_lstm_model, max_sequence_length, char_tokenizer)\n",
        "print(\"Predicted sequence:\", predicted_sequence)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
